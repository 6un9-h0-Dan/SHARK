{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlevental/miniconda3/envs/torch-mlir/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import torch\n",
    "from shark.iree_utils import get_iree_compiled_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# torch dynamo related imports\n",
    "import torchdynamo\n",
    "from torchdynamo.optimizations.backends import create_backend\n",
    "from torchdynamo.optimizations.subgraph import SubGraph\n",
    "\n",
    "# torch-mlir imports for compiling\n",
    "from torch_mlir import compile, OutputType"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[TorchDynamo](https://github.com/pytorch/torchdynamo) is a compiler for PyTorch programs that uses the [frame evaluation API](https://www.python.org/dev/peps/pep-0523/) in CPython to dynamically modify Python bytecode right before it is executed. It creates this FX Graph through bytecode analysis and is designed to mix Python execution with compiled backends."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def toy_example(*args):\n",
    "    a, b = args\n",
    "\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# compiler that lowers fx_graph to through MLIR\n",
    "def __torch_mlir(fx_graph, *args, **kwargs):\n",
    "    assert isinstance(\n",
    "        fx_graph, torch.fx.GraphModule\n",
    "    ), \"Model must be an FX GraphModule.\"\n",
    "\n",
    "    def _unwrap_single_tuple_return(fx_g: torch.fx.GraphModule):\n",
    "        \"\"\"Replace tuple with tuple element in functions that return one-element tuples.\"\"\"\n",
    "\n",
    "        for node in fx_g.graph.nodes:\n",
    "            if node.op == \"output\":\n",
    "                assert len(node.args) == 1, \"Output node must have a single argument\"\n",
    "                node_arg = node.args[0]\n",
    "                if isinstance(node_arg, tuple) and len(node_arg) == 1:\n",
    "                    node.args = (node_arg[0],)\n",
    "        fx_g.graph.lint()\n",
    "        fx_g.recompile()\n",
    "        return fx_g\n",
    "\n",
    "    fx_graph = _unwrap_single_tuple_return(fx_graph)\n",
    "    ts_graph = torch.jit.script(fx_graph)\n",
    "\n",
    "    # torchdynamo does munges the args differently depending on whether you use\n",
    "    # the @torchdynamo.optimize decorator or the context manager\n",
    "    if isinstance(args, tuple):\n",
    "        args = list(args)\n",
    "    assert isinstance(args, list)\n",
    "    if len(args) == 1 and isinstance(args[0], list):\n",
    "        args = args[0]\n",
    "\n",
    "    linalg_module = compile(ts_graph, args, output_type=OutputType.LINALG_ON_TENSORS)\n",
    "    callable, _ = get_iree_compiled_module(linalg_module, \"cuda\", func_name=\"forward\")\n",
    "\n",
    "    def forward(*inputs):\n",
    "        return callable(*inputs)\n",
    "\n",
    "    return forward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simplest way to use TorchDynamo with the `torchdynamo.optimize` context manager:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 device(s).\n",
      "Device: 0\n",
      "  Name: NVIDIA GeForce RTX 3080\n",
      "  Compute Capability: 8.6\n",
      "[ 0.40569967 -0.09795299  0.15944454 -0.11522183  0.13940483  0.6483943\n",
      "  0.04897427  0.20021795  0.4110793  -0.01060459]\n",
      "[-2.36014053e-01  1.02099776e-01 -8.32196176e-02  5.48950136e-01\n",
      " -1.22762606e-01  6.83019171e-05 -1.87891126e-01  2.95851409e-01\n",
      " -7.94005573e-01 -7.86187351e-02]\n",
      "[-0.08547013 -0.03790672 -0.67750883  0.07134506  0.48344284 -0.04401336\n",
      "  0.5358189   0.19252774  0.01672608  0.16548733]\n",
      "[ 0.16950132 -0.14072983  0.0850194   0.51586574  0.6814878   0.09228899\n",
      "  0.00628967  0.04618661  0.33402687  0.0672036 ]\n",
      "[-3.0006915e-01 -5.6649814e-03  1.0971012e-02  6.7839026e-01\n",
      "  1.4477329e-01  7.1921291e-05 -1.2694100e-01 -1.0598335e-01\n",
      "  4.5776103e-02 -3.7474141e-02]\n",
      "[ 0.1161904   0.11104004  0.03108321 -0.01897361 -0.2773486  -0.1210255\n",
      " -0.10480757  0.15325065  0.07355037  0.43414077]\n",
      "[-0.15983443  0.18079512 -0.05479247  0.06110435  0.12209348  0.12046977\n",
      "  0.20978567 -0.43570745 -0.90952724  0.06195822]\n",
      "[ 0.08218328 -0.34357572  0.89349353  0.08410293  0.02417435 -0.04034815\n",
      " -0.25662944  0.28095868 -0.03567748  0.21895115]\n",
      "[ 0.04629911 -0.23499016  0.00494049  0.7605684   0.36587524  0.38433665\n",
      " -0.04501833 -0.030997   -0.6260068   0.11544845]\n",
      "[-0.18239398 -0.02263366  0.14311954  0.07364512  0.16239333  0.02270873\n",
      " -0.07463308  0.9391377  -0.24326107  0.10405837]\n"
     ]
    }
   ],
   "source": [
    "with torchdynamo.optimize(__torch_mlir):\n",
    "    for _ in range(10):\n",
    "        print(toy_example(torch.randn(10), torch.randn(10)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can also be used through a decorator:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@create_backend\n",
    "def torch_mlir(subgraph, *args, **kwargs):\n",
    "    assert isinstance(subgraph, SubGraph), \"Model must be a dynamo SubGraph.\"\n",
    "    return __torch_mlir(subgraph.model, *list(subgraph.example_inputs))\n",
    "\n",
    "@torchdynamo.optimize(\"torch_mlir\")\n",
    "def toy_example2(*args):\n",
    "    a, b = args\n",
    "\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 device(s).\n",
      "Device: 0\n",
      "  Name: NVIDIA GeForce RTX 3080\n",
      "  Compute Capability: 8.6\n",
      "[-0.18768834 -0.13050991  0.16192573 -0.08606607 -0.7383352   0.21919324\n",
      "  0.1471572   0.1957912   0.78911537  0.3079384 ]\n",
      "[-0.6180763   0.3083624   0.30140203 -0.13603576  0.13938724  0.06829462\n",
      " -0.5264937   0.47425482  0.631954    0.07812975]\n",
      "[ 0.02927845 -0.21328457  0.03714288 -0.04158077  0.00811315  0.06475347\n",
      "  0.16807333 -0.2835701   0.07122174 -0.25891435]\n",
      "[-0.7808262  -0.00184676 -0.42828155  0.02376047  0.23778288 -0.2332218\n",
      "  0.35119227 -0.45859754 -0.16244853  0.08230756]\n",
      "[ 0.65825784 -0.43039966  0.49089798  0.16756855  0.17000133  0.1523097\n",
      "  0.00477562  0.05351321 -0.16297375  0.42369154]\n",
      "[-0.35411894  0.34467864  0.19818862 -0.26733887 -0.36235648  1.570275\n",
      "  0.08005163 -0.00406713 -0.7041876   0.2678179 ]\n",
      "[-0.8275841   0.01846866  0.27031392  0.07428868 -0.3472132   0.72673005\n",
      "  0.04936812 -0.0711254  -0.15575752 -0.1963107 ]\n",
      "[0.2804986  0.2999007  0.59554356 0.00538198 0.06104416 0.40309286\n",
      " 0.2739955  0.0901546  0.43381387 0.16430663]\n",
      "[-0.45742902  0.3646233   0.6719164   1.2339892   0.16049416  0.50693405\n",
      " -0.50248224  0.03397409 -0.6752035   0.22877756]\n",
      "[ 0.10492023  0.03040915  0.22470844  0.6017106   0.00301571  0.16422868\n",
      " -0.111872    0.12692471 -0.50792587  0.3374479 ]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(toy_example2(torch.randn(10), torch.randn(10)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}